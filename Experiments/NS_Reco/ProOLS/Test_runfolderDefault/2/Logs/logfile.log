Module path:  Environments.NS_Reco NS_Reco
Dynamically loaded from:  <class 'Environments.NS_Reco.NS_Reco'>
Reward Amplitudes: [4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01
 1.46755891e-01] :: Avg 0.317309867282206 
Module path:  Src.Algorithms.ProOLS ProOLS
Dynamically loaded from:  <class 'Src.Algorithms.ProOLS.ProOLS'>
=====Configurations=====
 Namespace(NN_basis_dim='32', Policy_basis_dim='32', actor_lr=0.01, algo_name='ProOLS', base=0, batch_size=1000, buffer_size=1000, debug=True, delta=5, entropy_lambda=0.1, env_name='NS_Reco', experiment='Test_runfolder', extrapolator_basis='Poly', folder_suffix='Default', fourier_coupled=True, fourier_k=7, fourier_order=-1, gamma=0.99, gauss_std=1.5, gpu=0, hyper='default', importance_clip=10.0, inc=1, log_output='term_file', max_episodes=3000, max_inner=150, max_steps=500, optim='rmsprop', oracle=-1000, raw_basis=True, restore=False, save_count=10, save_model=True, seed=2, speed=2, state_lr=0.001, summary=True, swarm=False, timestamp='9|21|14:34:41')
Actions space: 5 :: State space: 1
State Low: tensor([0.]) :: State High: tensor([1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([5, 1])), ('fc1.bias', torch.Size([5]))]
0 :: Rewards -0.012 :: steps: 0.00 :: Time: 0.000(0.00181/step) :: Entropy : 0.000 :: Grads : [[], []]
300 :: Rewards 120.888 :: steps: 1.00 :: Time: 0.015(0.01538/step) :: Entropy : 0.000 :: Grads : [[0.0014064934, 0.0014064934], []]
600 :: Rewards 170.580 :: steps: 1.00 :: Time: 0.020(0.02011/step) :: Entropy : 0.000 :: Grads : [[0.011903539, 0.011903539], []]
900 :: Rewards 143.363 :: steps: 1.00 :: Time: 0.026(0.02556/step) :: Entropy : 0.000 :: Grads : [[0.0021888632, 0.0021888632], []]
1200 :: Rewards 302.031 :: steps: 1.00 :: Time: 0.029(0.02912/step) :: Entropy : 0.000 :: Grads : [[0.0010629208, 0.0010629208], []]
1500 :: Rewards 404.573 :: steps: 1.00 :: Time: 0.029(0.02883/step) :: Entropy : 0.000 :: Grads : [[0.0194195, 0.0194195], []]
1800 :: Rewards 446.352 :: steps: 1.00 :: Time: 0.030(0.02991/step) :: Entropy : 0.000 :: Grads : [[0.0015493531, 0.0015493531], []]
2100 :: Rewards 611.490 :: steps: 1.00 :: Time: 0.030(0.02952/step) :: Entropy : 0.000 :: Grads : [[0.00046047647, 0.00046047647], []]
2400 :: Rewards 634.657 :: steps: 1.00 :: Time: 0.027(0.02676/step) :: Entropy : 0.000 :: Grads : [[0.0014917279, 0.0014917279], []]
2700 :: Rewards 725.144 :: steps: 1.00 :: Time: 0.029(0.02929/step) :: Entropy : 0.000 :: Grads : [[0.0977538, 0.0977538], []]
2999 :: Rewards 888.546 :: steps: 1.00 :: Time: 0.029(0.02907/step) :: Entropy : 0.000 :: Grads : [[0.0078317085, 0.0078317085], []]
Total time taken: 80.41333937644958
