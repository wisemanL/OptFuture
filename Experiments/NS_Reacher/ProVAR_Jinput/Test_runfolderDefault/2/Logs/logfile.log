Module path:  Environments.NS_Reacher NS_Reacher
Dynamically loaded from:  <class 'Environments.NS_Reacher.NS_Reacher'>
Module path:  Src.Algorithms.ProVAR_Jinput ProVAR_Jinput
Dynamically loaded from:  <class 'Src.Algorithms.ProVAR_Jinput.ProVAR_Jinput'>
=====Configurations=====
 Namespace(NN_basis_dim='32', Policy_basis_dim='32', actor_lr=0.01, algo_name='ProVAR_Jinput', base=0, batch_size=1000, buffer_size=1000, debug=True, delta=1, entropy_lambda=0.1, env_name='NS_Reacher', experiment='Test_runfolder', extrapolator_basis='Poly', folder_suffix='Default', fourier_coupled=True, fourier_k=4, fourier_order=-1, gamma=0.99, gauss_std=1.5, gpu=0, hyper='default', importance_clip=10.0, inc=1, log_output='term_file', max_episodes=1000, max_inner=150, max_steps=500, optim='rmsprop', oracle=-1000, raw_basis=True, restore=False, save_count=10, save_model=True, seed=2, speed=2, state_lr=0.001, summary=True, swarm=False, timestamp='10|11|15:0:10')
Actions space: 4 :: State space: 2
State Low: tensor([0., 0.]) :: State High: tensor([1., 1.])
State features:  [('dummy_param', torch.Size([1]))]
Policy:  [('fc1.weight', torch.Size([4, 2])), ('fc1.bias', torch.Size([4]))]
0
0 :: Rewards -6.000 :: steps: 0.13 :: Time: 0.000(0.00084/step) :: Entropy : 0.000 :: Grads : [[], []]
1
2
3
4
